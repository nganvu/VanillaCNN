#!/usr/bin/python3

import sys, os
import numpy as np

# Load MNISt dataset.
sys.stderr = open(os.devnull, 'w') # Disable stderr.
from keras.datasets import mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
sys.stderr = sys.__stderr__ # Enable stderr.

# Convert labels to one-hot vectors.
def convert_to_one_hot(labels):
	labels_vecs = np.zeros((labels.size, 10))
	labels_vecs[np.arange(labels.size), labels] = 1
	return labels_vecs

y_train = convert_to_one_hot(y_train)
y_test = convert_to_one_hot(y_test)

# Scale a set uniformly sampled from [0, 1] to [-bound, bound].
def scale_to_bound(arr, arr_bound):
	return arr * 2 * arr_bound - arr_bound

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FEEDFORWARD ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

#################################
##### Convolutional Layer 1 #####
#################################
# (1 x 6) kernels of size (5 x 5).
K1 = np.random.rand(1, 6, 5, 5)
K1_bound = np.sqrt(6 / ((1 + 6) * (5 * 5)))
K1 = scale_to_bound(K1, K1_bound)

# 6 biases.
B1 = np.zeros((6, 1))

# 6 convolution outputs of size (24 x 24).
C1 = np.zeros((6, 24, 24))

#################################
#####    Pooling Layer 1    #####
#################################
# 6 sampling outputs of size (12 x 12).
S1 = np.zeros((6, 12, 12))

#################################
##### Convolutional Layer 2 #####
#################################
# (6 x 12) kernels of size (5 x 5).
K2 = np.random.rand(6, 12, 5, 5)
K2_bound = np.sqrt(6 / ((6 + 12) * (5 * 5)))
K2 = scale_to_bound(K2, K2_bound)

# 12 biases.
B2 = np.zeros((12, 1))

# 12 convolution outputs of size (8 x 8).
C2 = np.zeros((12, 8, 8))

#################################
#####    Pooling Layer 2    #####
#################################
# 12 sampling outputs of size (4 x 4).
S2 = np.zeros((12, 4, 4))

#################################
##### Fully Connected Layer #####
#################################
# Vectorization of 12 matrices of size (4 x 4) to a vector of size 192.
F = np.zeros((192, 1))

# 10 weights of size 192.
W = np.random.rand(10, 192)
W_bound = np.sqrt(6 / (192 + 10))
W = scale_to_bound(W, W_bound)

# 10 biases.
B = np.zeros((10, 1))

# 10 output classes.
Y = np.zeros((10, 1))

#################################
#####      Convolution      #####
#################################
def convolution(in_mat, kernel, out_mat):
	u_length, v_length = kernel.shape

	# Make sure the shape of output matches the convolution of input and kernel.
	# Only keep the parts of input that are computed without zero-padded edges.
	xo_length, yo_length = out_mat.shape
	xi_length, yi_length = in_mat.shape
	assert(xi_length == xo_length + u_length - 1)
	assert(yi_length == yo_length + v_length - 1)

	u_margin = int(u_length / 2)
	v_margin = int(v_length / 2)

	res = np.zeros(in_mat.shape)

	# Iterate through output image.
	for i in range(xi_length):
		for j in range(yi_length):
			# Iterate through kernel.
			for u in range(u_length):
				for v in range(v_length):
					# TODO: Double check the -1 part. Might not need it.
					iu = i - (u - u_margin) - (1 if u_length % 2 == 0 else 0)
					jv = j - (v - v_margin) - (1 if v_length % 2 == 0 else 0)

					# Ignore parts that require zero-padded edges.
					if ((iu < 0 or iu >= xi_length) or
						(jv < 0 or jv >= yi_length)):
						continue
					else:
						res[i][j] += in_mat[iu][jv] * kernel[u][v]
	u_end = u_length - u_margin - 1
	v_end = v_length - v_margin - 1
	res = res[u_margin:(-u_end if u_end > 0 else None),
			  v_margin:(-v_end if v_end > 0 else None)]
	assert(out_mat.shape == res.shape)
	return res

#################################
#####        Sigmoid        #####
#################################
def sigmoid(in_mat):
	return 1 / (1 + np.exp(-in_mat))

#################################
#####        Pooling        #####
#################################
def pooling(in_mat, out_mat):
	# Make sure the side length of output is half the side length of the input.
	# Assume the side length of the input is even.
	xo_length, yo_length = out_mat.shape
	xi_length, yi_length = in_mat.shape
	assert(xi_length % 2 == 0 and yi_length % 2 == 0)
	assert(xi_length == 2 * xo_length)
	assert(yi_length == 2 * yo_length)

	# Iterate through the output image.
	for i in range(xo_length):
		for j in range(yo_length):
			out_mat[i][j] = 0.25 * (in_mat[2*i    ][2*j    ] +
									in_mat[2*i + 1][2*j    ] +
									in_mat[2*i    ][2*j + 1] +
									in_mat[2*i + 1][2*j + 1])
	return out_mat

#################################
#####     Vectorization     #####
#################################
def vectorize(mats):
	return mats.reshape(-1, 1)

def unvectorize(vec):
	return vec.reshape(S2.shape)

#################################
#####    Fully Connected    #####
#################################
def fully_connected(F, W, B):
	return sigmoid(np.dot(W, F) + B)

#################################
#####     Loss Function     #####
#################################
def loss(truth, prediction):
	return 0.5 * np.sum(np.square((truth - prediction)))

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ BACKPROPAGATION ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

def find_delta_y_hat(y, y_hat):
	delta_y_hat = np.multiply((y_hat - y), np.multiply((y_hat), (1 - y_hat)))
	assert(delta_y_hat.shape == y_hat.shape)
	return delta_y_hat

def find_delta_W(delta_y_hat, F):
	delta_W = delta_y_hat.dot(F.T)
	assert(delta_W.shape == W.shape)
	return delta_W

def find_delta_B(delta_y_hat):
	delta_B = delta_y_hat
	assert(delta_B.shape == B.shape)
	return delta_B

def find_delta_F(W, delta_y_hat):
	delta_F = W.T.dot(delta_y_hat)
	assert(delta_F.shape == F.shape)
	return delta_F

def find_delta_S2(delta_F):
	delta_S2 = unvectorize(delta_F)
	assert(delta_S2.shape == S2.shape)
	return delta_S2

def find_delta_C2(delta_S2):
	delta_C2 = np.zeros(C2.shape)
	for q in range(C2.shape[0]):
		for i in range(C2.shape[1]):
			for j in range(C2.shape[2]):
				delta_C2[q][i][j] = 0.25 * delta_S2[q][int(i / 2)][int(j / 2)]
	assert(delta_C2.shape==C2.shape)
	return delta_C2

def find_delta_C2_sigma(delta_C2, C2):
	delta_C2_sigma = np.multiply(delta_C2, np.multiply(C2, 1 - C2))
	assert(delta_C2_sigma.shape == C2.shape)
	return delta_C2_sigma

def find_delta_K2(delta_C2_sigma, S1):
	delta_K2 = np.zeros(K2.shape)
	for p in range(K2.shape[0]):
		for q in range(K2.shape[1]):
			S1_p_rot = np.rot90(S1[p], 2)
			# TODO: Fix convolution to support matrix with even side length.
			# TODO: Reprove this part in the paper; seems very handwavy.
			# delta_K2[p][q] = \
			# 	convolution(np.rot90(S1[p], 2),
			# 				delta_C2_sigma[q],
			# 				delta_K2[p][q])
			for u in range(K2.shape[2]):
				for v in range(K2.shape[3]):
					delta_K2[p][q][u][v] = 0
					for i in range(C2[0].shape[0]):
						for j in range(C2[0].shape[1]):
							if (u - i < 0 or v - j < 0):
								continue
							else:
								delta_K2[p][q][u][v] += S1_p_rot[u - i][v - j] \
									* delta_C2_sigma[q][i][j]
	assert(delta_K2.shape == K2.shape)
	return delta_K2

def find_delta_B2(delta_C2_sigma):
	delta_B2 = np.zeros(B2.shape)
	for q in range(B2.shape[0]):
		delta_B2[q] = np.sum(delta_C2_sigma[q])
	assert(delta_B2.shape==B2.shape)
	return delta_B2

def find_delta_S1(delta_C2_sigma, K2):
	delta_S1 = np.zeros(S1.shape)
	for p in range(delta_S1.shape[0]):
		temp = np.zeros(delta_S1[p].shape)
		for q in range(delta_C2_sigma.shape[0]):
			# TODO: Reprove this part in the paper. Clarify why we need padding.
			temp = convolution(np.pad(delta_C2_sigma[q], 4, 'constant'),
												 np.rot90(K2[p][q], 2),
												 temp)
			delta_S1[p] += temp
	assert(delta_S1.shape == S1.shape)
	return delta_S1

def find_delta_C1(delta_S1):
	delta_C1 = np.zeros(C1.shape)
	for q in range(C1.shape[0]):
		for i in range(C1.shape[1]):
			for j in range(C1.shape[2]):
				delta_C1[q][i][j] = 0.25 * delta_S1[q][int(i / 2)][int(j / 2)]
	assert(delta_C1.shape==C1.shape)
	return delta_C1

def find_delta_C1_sigma(delta_C1, C1):
	delta_C1_sigma = np.multiply(delta_C1, np.multiply(C1, 1 - C1))
	assert(delta_C1_sigma.shape == C1.shape)
	return delta_C1_sigma

def find_delta_K1(delta_C1_sigma, I):
	delta_K1 = np.zeros(K1.shape)
	I_rot = np.rot90(I, 2)
	for p in range(K1.shape[0]):
		for q in range(K1.shape[1]):
			# TODO: Fix convolution to support matrix with even side length.
			# TODO: Reprove this part in the paper; seems very handwavy.
			# delta_K1[p][q] = \
			# 	convolution(np.rot90(S1[p], 2), delta_C2_sigma[q], delta_K1[p][q])
			for u in range(K1.shape[2]):
				for v in range(K1.shape[3]):
					delta_K1[p][q][u][v] = 0
					for i in range(C1[0].shape[0]):
						for j in range(C1[0].shape[1]):
							if (u - i < 0 or v - j < 0):
								continue
							else:
								delta_K1[p][q][u][v] += \
									I_rot[u - i][v - j] * \
									delta_C1_sigma[q][i][j]
	assert(delta_K1.shape == K1.shape)
	return delta_K1

def find_delta_B1(delta_C1_sigma):
	delta_B1 = np.zeros(B1.shape)
	for q in range(B1.shape[0]):
		delta_B1[q] = np.sum(delta_C1_sigma[q])
	assert(delta_B1.shape==B1.shape)
	return delta_B1

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~ UPDATING PARAMETERS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

def update_parameters(
	K1, delta_K1, B1, delta_B1,
	K2, delta_K2, B2, delta_B2,
	W, delta_W, B, delta_B,
	alpha):
	K1 = K1 - alpha * delta_K1
	B1 = B1 - alpha * delta_B1
	K2 = K2 - alpha * delta_K2
	B2 = B2 - alpha * delta_B2
	W = W - alpha * delta_W
	B = B - alpha * delta_B
	return (K1, B1, K2, B2, W, B)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ TESTING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

# Component tests, with manual inputs starting from S1.
# All types of layers are covered.
# Test convolution.
def baseline_convolution(in_mat, kernel, out_mat):
    s = kernel.shape + tuple(np.subtract(in_mat.shape, kernel.shape) + 1)
    subM = np.lib.stride_tricks.as_strided(
		in_mat, shape = s, strides = in_mat.strides * 2)
    return np.einsum("ij,ijkl->kl", kernel, subM)

print("First convolution test: Convolve with delta function")
in_mat = np.ones((12, 12))
out_mat = np.zeros((8, 8))
kernel = np.zeros((5, 5))
kernel[2][2] = 1
print("Input")
print(in_mat)
print("Kernel")
print(kernel)
print("Baseline:")
print(baseline_convolution(in_mat, kernel, out_mat))
print("Mine:")
print(convolution(in_mat, kernel, out_mat))

print("Second convolution test: Integer, odd-side-length square matrices")
in_mat = np.ones((5, 5))
out_mat = np.zeros((3, 3))
kernel = np.zeros((3, 3))
kernel[1][1] = 2
kernel[0][1] = kernel[2][1] = kernel[1][0] = kernel[1][2] = 1
print("Input")
print(in_mat)
print("Kernel")
print(kernel)
print("Baseline:")
print(baseline_convolution(in_mat, kernel, out_mat))
print("Mine:")
print(convolution(in_mat, kernel, out_mat))

print("Third convolution test: Random, odd-side-length square matrices")
in_mat = np.random.rand(5, 5)
out_mat = np.zeros((3, 3))
kernel = np.random.rand(3, 3)
print("Input")
print(in_mat)
print("Kernel")
print(kernel)
# The baseline is actually correlation, not convolution!!!
print("Baseline:")
print(baseline_convolution(in_mat, np.rot90(kernel, 2), out_mat))
print("Mine:")
print(convolution(in_mat, kernel, out_mat))

print("Fourth convolution test: Random, any size matrices")
in_mat = np.random.rand(5, 4)
out_mat = np.zeros((4, 2))
kernel = np.random.rand(2, 3)
kernel[0][0] = 1
print("Input")
print(in_mat)
print("Kernel")
print(kernel)
# The baseline is actually correlation, not convolution!!!
print("Baseline:")
print(baseline_convolution(in_mat, np.rot90(kernel, 2), out_mat))
print("Mine:")
print(convolution(in_mat, kernel, out_mat))

sys.exit()

# Check for syntatical correctness (not logical correctness yet).
BATCH_SIZE = 100
EPOCHS = 200
print("Batch size: {}, Epochs: {}".format(BATCH_SIZE, EPOCHS))

for e in range(EPOCHS):
	print("Epoch {}".format(e))

	avg_delta_K1 = np.zeros(K1.shape)
	avg_delta_B1 = np.zeros(B1.shape)
	avg_delta_K2 = np.zeros(K2.shape)
	avg_delta_B2 = np.zeros(B2.shape)
	avg_delta_W = np.zeros(W.shape)
	avg_delta_B = np.zeros(B.shape)

	for s in range(BATCH_SIZE):
		I = x_train[e * BATCH_SIZE + s]
		y = y_train[e * BATCH_SIZE + s]

		# Convolution 1
		for i in range(C1.shape[0]):
			C1[i] = convolution(I, K1[0][i], C1[i])
			C1[i] = sigmoid(C1[i] + B1[i])

		# Pooling 1
		for i in range(S1.shape[0]):
			S1[i] = pooling(C1[i], S1[i])

		# Convolution 2
		for i in range(C2.shape[0]):
			temp = np.zeros(C2[i].shape)
			for j in range(S1.shape[0]):
				temp = convolution(S1[j], K2[j][i], temp)
				C2[i] += temp
			C2[i] = sigmoid(C2[i] + B2[i])

		# Pooling 2
		for i in range(S2.shape[0]):
			S2[i] = pooling(C2[i], S2[i])

		# Vectorization
		F = vectorize(S2)

		# Fully Connected
		y_hat = sigmoid(fully_connected(F, W, B))

		if s % 10 == 0:
			print("Loss: {}".format(loss(y, y_hat)))

		delta_y_hat = find_delta_y_hat(y.reshape(10, 1), y_hat)
		delta_W = find_delta_W(delta_y_hat, F)
		delta_B = find_delta_B(delta_y_hat)

		delta_F = find_delta_F(W, delta_y_hat)

		delta_S2 = find_delta_S2(delta_F)
		delta_C2 = find_delta_C2(delta_S2)
		delta_C2_sigma = find_delta_C2_sigma(delta_C2, C2)
		delta_K2 = find_delta_K2(delta_C2_sigma, S1)
		delta_B2 = find_delta_B2(delta_C2_sigma)

		delta_S1 = find_delta_S1(delta_C2_sigma, K2)
		delta_C1 = find_delta_C1(delta_S1)
		delta_C1_sigma = find_delta_C1_sigma(delta_C1, C1)
		delta_K1 = find_delta_K1(delta_C1_sigma, I)
		delta_B1 = find_delta_B1(delta_C1_sigma)

		avg_delta_K1 += delta_K1
		avg_delta_B1 += delta_B1
		avg_delta_K2 += delta_K2
		avg_delta_B2 += delta_B2
		avg_delta_W += delta_W
		avg_delta_B += delta_B

	avg_delta_K1 /= BATCH_SIZE
	avg_delta_B1 /= BATCH_SIZE
	avg_delta_K2 /= BATCH_SIZE
	avg_delta_B2 /= BATCH_SIZE
	avg_delta_W /= BATCH_SIZE
	avg_delta_B /= BATCH_SIZE

	K1, B1, K2, B2, W, B = update_parameters(K1, avg_delta_K1,
											 B1, avg_delta_B1,
											 K2, avg_delta_K2,
											 B2, avg_delta_B2,
											 W, avg_delta_W,
											 B, avg_delta_B,
											 0.2)

# Finished training. Try on first 20 sampes.
correct = 0
incorrect = 0
for s in range(20):
	if s % 50 == 0:
		print("Epoch {}".format(s))
	I = x_train[s]
	y = y_train[s]

	# Convolution 1
	for i in range(C1.shape[0]):
		C1[i] = convolution(I, K1[0][i], C1[i])
		C1[i] = sigmoid(C1[i] + B1[i])

	# Pooling 1
	for i in range(S1.shape[0]):
		S1[i] = pooling(C1[i], S1[i])

	# Convolution 2
	for i in range(C2.shape[0]):
		temp = np.zeros(C2[i].shape)
		for j in range(S1.shape[0]):
			temp = convolution(S1[j], K2[j][i], temp)
			C2[i] += temp
		C2[i] = sigmoid(C2[i] + B2[i])

	# Pooling 2
	for i in range(S2.shape[0]):
		S2[i] = pooling(C2[i], S2[i])

	# Vectorization
	F = vectorize(S2)

	# Fully Connected
	y_hat = sigmoid(fully_connected(F, W, B))

	# Prediction:
	p = np.argmax(y_hat)
	a = np.argmax(y)
	print("Predicted: {} - {}".format(p, y_hat))
	print("Actual: {} - {}".format(a, y))
	if p == a:
		correct += 1
	else:
		incorrect += 1

print(correct)
print(incorrect)
