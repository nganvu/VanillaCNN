#!/usr/bin/python3

import numpy as np

# Load MNISt dataset.
from keras.datasets import mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Convert labels to one-hot vectors.
def convert_to_one_hot(labels):
	labels_vecs = np.zeros((labels.size, 10))
	labels_vecs[np.arange(labels.size), labels] = 1
	return labels_vecs

y_train = convert_to_one_hot(y_train)
y_test = convert_to_one_hot(y_test)

# Scale a set uniformly sampled from [0, 1] to [-bound, bound].
def scale_to_bound(arr, arr_bound):
	return arr * 2 * arr_bound - arr_bound

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FEEDFORWARD ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#################################
##### Convolutional Layer 1 #####
#################################
# (1 x 6) kernels of size (5 x 5).
K1 = np.random.rand(1, 6, 5, 5)
K1_bound = np.sqrt(6 / ((1 + 6) * (5 * 5)))
K1 = scale_to_bound(K1, K1_bound)

# 6 biases.
B1 = np.zeros((6, 1))

# 6 convolution outputs of size (24 x 24).
C1 = np.zeros((6, 24, 24))

#################################
#####    Pooling Layer 1    #####
#################################
# 6 sampling outputs of size (12 x 12).
S1 = np.zeros((6, 12, 12))

#################################
##### Convolutional Layer 2 #####
#################################
# (6 x 12) kernels of size (5 x 5).
K2 = np.random.rand(6, 12, 5, 5)
K2_bound = np.sqrt(6 / ((6 + 12) * (5 * 5)))
K2 = scale_to_bound(K2, K2_bound)

# 12 biases.
B2 = np.zeros((12, 1))

# 12 convolution outputs of size (8 x 8).
C2 = np.zeros((12, 8, 8))

#################################
#####    Pooling Layer 2    #####
#################################
# 12 sampling outputs of size (4 x 4).
S2 = np.zeros((12, 4, 4))

#################################
##### Fully Connected Layer #####
#################################
# Vectorization of 12 matrices of size (4 x 4) to a vector of size 192.
F = np.zeros((192, 1))

# 10 weights of size 192.
W = np.random.rand(10, 192)
W_bound = np.sqrt(6 / (192 + 10))
W = scale_to_bound(W, W_bound)

# 10 biases.
B = np.zeros((10, 1))

# 10 output classes.
Y = np.zeros((10, 1))

#################################
#####      Convolution      #####
#################################
def convolution(in_mat, kernel, out_mat):
	# Assume the kernel is square and has an odd side length.
	u_length, v_length = kernel.shape
	assert(u_length == v_length and u_length % 2 == 1)

	# Make sure the shape of output matches the convolution of input and kernel.
	# Only keep the parts of input that are computed without zero-padded edges.
	xo_length, yo_length = out_mat.shape
	xi_length, yi_length = in_mat.shape
	assert(xi_length == xo_length + u_length - 1)
	assert(yi_length == yo_length + v_length - 1)

	u_margin = v_margin = int(u_length / 2)
	(u_lbound, u_ubound) = (v_lbound, v_ubound) = (-u_margin, u_margin)

	# Iterate through output image.
	for i in range(xo_length):
		for j in range(yo_length):
			# Iterate through kernel.
			for u in range(u_lbound, u_ubound):
				for v in range(v_lbound, v_ubound):
					# Ignore parts that require zero-padded edges.
					if ((i - u < 0 or i - u >= xi_length) or
						(j - v < 0 or j - v >= yi_length)):
						continue
					else:
						out_mat[i][j] = in_mat[i - u][j - v] * \
							kernel[u + u_margin][v + v_margin]

#################################
#####        Sigmoid        #####
#################################
def sigmoid(in_mat):
	return 1 / (1 + np.exp(-in_mat))

#################################
#####        Pooling        #####
#################################
def pooling(in_mat, out_mat):
	# Make sure the side length of output is half the side length of the input.
	# Assume the side length of the input is even.
	xo_length, yo_length = out_mat.shape
	xi_length, yi_length = in_mat.shape
	assert(xi_length % 2 == 0 and yi_length % 2 == 0)
	assert(xi_length == 2 * xo_length)
	assert(yi_length == 2 * yo_length)

	# Iterate through the output image.
	for i in range(xo_length):
		for j in range(yo_length):
			out_mat[i][j] = 0.25 * (in_mat[2*i    ][2*j    ] + 
															in_mat[2*i + 1][2*j    ] + 
															in_mat[2*i    ][2*j + 1] +
															in_mat[2*i + 1][2*j + 1])

#################################
#####     Vectorization     #####
#################################
def vectorize(mats):
	return mats.reshape(-1, 1)

def unvectorize(vec):
	return vec.reshape(S2.shape)

#################################
#####    Fully Connected    #####
#################################
def fully_connected(F, W, B):
	return sigmoid(np.dot(W, F) + B)

#################################
#####     Loss Function     #####
#################################
def loss(truth, prediction):
	return 0.5 * np.square(np.sum((truth - prediction)))

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ BACKPROPAGATION ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

def find_delta_y_hat(y, y_hat):
	return np.multiply((y_hat - y), np.multiply((y_hat), (1 - y_hat)))

def find_delta_W(delta_y_hat, F):
	return delta_y_hat.dot(F.T)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ TESTING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Check for syntatical correctness (not logical correctness yet).
I = x_train[0]

print("Convolution 1")
for i in range(C1.shape[0]):
	convolution(I, K1[0][i], C1[i])
	C1[i] = sigmoid(C1[i] + B1[i])

print("Pooling 1")
for i in range(S1.shape[0]):
	pooling(C1[i], S1[i])

print("Convolution 2")
for i in range(C2.shape[0]):
	temp = np.zeros(C2[i].shape)
	for j in range(S1.shape[0]): 
		convolution(S1[j], K2[j][i], temp)
		C2[i] += temp
	C2[i] = sigmoid(C2[i] + B2[i])

print("Pooling 2")
for i in range(S2.shape[0]):
	pooling(C2[i], S2[i])

print("Vectorization")
F = vectorize(S2)
print(F.shape)

# S2 should stay the same after vectorization and unvectorization.
S2_new = unvectorize(F)
print(S2_new.shape)
print(np.all(np.equal(S2, S2_new)))

print("Fully Connected")
y_hat = fully_connected(F, W, B)

print(loss(y_train[0], y_hat))

delta_y_hat = find_delta_y_hat(y_train[0].reshape(10, 1), y_hat)
delta_W = find_delta_W(delta_y_hat, F)
