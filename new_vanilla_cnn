#!/usr/bin/python3

import sys, os
import numpy as np

class CNN:

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#~~~~~~~~~~~~~~~~~~~~~~~~~~ INITIALIZING PARAMETERS ~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

  # Load MNIST dataset.
  def load_MNIST(self):
    sys.stderr = open(os.devnull, 'w') # Disable stderr.
    from keras.datasets import mnist
    (self.X_train, self.Y_train), (self.X_test, self.Y_test) = mnist.load_data()
    self.Y_train = self.convert_to_one_hot(self.Y_train)
    self.Y_test = self.convert_to_one_hot(self.Y_test)
    sys.stderr = sys.__stderr__ # Enable stderr.

  # Convert labels to one-hot (10 x 1) vectors.
  def convert_to_one_hot(self, labels):
      labels_vecs = np.zeros((labels.size, 10, 1))
      labels_vecs[np.arange(labels.size), labels] = 1
      return labels_vecs

  # Initialize an array of a certain shape so that its values are uniformly
  # distributed on [-bound, bound].
  def initialize_uniformly(self, shape, bound):
    return self.scale_to_bound(np.random.rand(*shape), bound)

  # Given an array of values uniformly distributed on [0, 1], scale these values
  # so that they are uniformly distributed on [-bound, bound].
  def scale_to_bound(self, arr, bound):
    return arr * 2 * bound - bound

  def __init__(self):
    ###########################################
    #####           Load Inputs           #####
    ###########################################
    self.X_train = None
    self.Y_train = None
    self.X_test = None
    self.Y_test = None
    self.load_MNIST()

    # Input (image of digits) and true output (label) holders.
    self.X = np.zeros(self.X_train[0].shape)
    self.Y = np.zeros(self.Y_train[0].shape)
    self.X = np.expand_dims(self.X, axis=0)

    ###########################################
    #####      Convolutional Layer 1      #####
    ###########################################
    # (1 x 6) kernels of size (5 x 5).
    K1_shape = (1, 6, 5, 5)
    K1_bound = np.sqrt(6 / ((1 + 6) * (5 * 5)))
    self.K1 = self.initialize_uniformly(K1_shape, K1_bound)
    self.delta_K1 = np.zeros(self.K1.shape)

    # 6 biases.
    self.B1 = np.zeros((6, 1))
    self.delta_B1 = np.zeros(self.B1.shape)

    # 6 convolution outputs of size (24 x 24), before and after sigmoid.
    self.C1 = np.zeros((6, 24, 24))
    self.C1_sigma = np.zeros(self.C1.shape)
    self.delta_C1 = np.zeros(self.C1.shape)
    self.delta_C1_sigma = np.zeros(self.C1_sigma.shape)

    ###########################################
    #####         Pooling Layer 1         #####
    ###########################################
    # 6 sampling outputs of size (12 x 12).
    self.S1 = np.zeros((6, 12, 12))
    self.delta_S1 = np.zeros(self.S1.shape)

    ###########################################
    #####      Convolutional Layer 2      #####
    ###########################################
    # (6 x 12) kernels of size (5 x 5).
    K2_shape = (6, 12, 5, 5)
    K2_bound = np.sqrt(6 / ((6 + 12) * (5 * 5)))
    self.K2 = self.initialize_uniformly(K2_shape, K2_bound)
    self.delta_K2 = np.zeros(self.K2.shape)

    # 12 biases.
    self.B2 = np.zeros((12, 1))
    self.delta_B2 = np.zeros(self.B2.shape)

    # 12 convolution outputs of size (8 x 8), before and after sigmoid.
    self.C2 = np.zeros((12, 8, 8))
    self.C2_sigma = np.zeros(self.C2.shape)
    self.delta_C2 = np.zeros(self.C2.shape)
    self.delta_C2_sigma = np.zeros(self.C2_sigma.shape)

    ###########################################
    #####         Pooling Layer 2         #####
    ###########################################
    # 12 sampling outputs of size (4 x 4).
    self.S2 = np.zeros((12, 4, 4))
    self.delta_S2 = np.zeros(self.S2.shape)

    ###########################################
    #####      Fully Connected Layer      #####
    ###########################################
    # Vectorization of 12 matrices of size (4 x 4) to a vector of size 192.
    self.V = np.zeros((192, 1))
    self.delta_V = np.zeros(self.V.shape)

    # 10 weights of size 192.
    W_shape = (10, 192)
    W_bound = np.sqrt(6 / (192 + 10))
    self.W = self.initialize_uniformly(W_shape, W_bound)
    self.delta_W = np.zeros(self.W.shape)

    # 10 biases.
    self.B = np.zeros((10, 1))
    self.delta_B = np.zeros(self.B.shape)

    # 10 output classes, before and after sigmoid.
    self.Y_hat = np.zeros((10, 1))
    self.Y_hat_sigma = np.zeros(self.Y_hat.shape)
    self.delta_Y_hat = np.zeros(self.Y_hat.shape)
    self.delta_Y_hat_sigma = np.zeros(self.delta_Y_hat.shape)

    # Loss.
    self.loss = 0.0

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~ UNIVERSAL FUNCTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

  ###########################################
  #####           Correlation           #####
  #####      (padding is not used)      #####
  ###########################################

  def correlation(self, input_name, kernel_name, bias_name, output_name):

    # Get input, kernel, bias, and output.
    all_I = getattr(self, input_name)
    all_K = getattr(self, kernel_name)
    all_B = getattr(self, bias_name)
    all_O = getattr(self, output_name)

    num_I = all_I.shape[0]
    num_K = (all_K.shape[0], all_K.shape[1])
    num_B = all_B.shape[0]
    num_O = all_O.shape[0]
    assert(num_I == num_K[0])
    assert(num_O == num_K[1] == num_B)
    assert(all_B.shape[1] == 1)

    for p in range(num_I):
      for q in range(num_O):
        I = all_I[p]
        K = all_K[p][q]
        B = all_B[q]
        O = all_O[q]

        # REFERENCE FORMULA: (5)
        # Verify the shape requirements for correlation with no padding.
        x_i, y_i = I.shape
        x_k, y_k = K.shape
        x_o, y_o = O.shape
        assert(x_o == x_i - x_k + 1)
        assert(y_o == y_i - y_k + 1)

        # REFERENCE FORMULA: (1)
        # Iterate through output matrix.
        for i in range(x_o):
          for j in range(y_o):
            O[i][j] = 0
            # Iterate through kernel.
            for u in range(x_k):
              for v in range(y_k):
                O[i][j] += I[i + u][j + v] * K[u][v]

        all_O[q] = O

    # Update output.
    setattr(self, output_name, all_O)

  ###########################################
  #####           Convolution           #####
  #####        (padding is used)        #####
  ###########################################

  def convolution(self, input_name, kernel_name, output_name):

    # Get input, kernel, bias, and output.
    all_I = getattr(self, input_name)
    all_K = getattr(self, kernel_name)
    all_B = getattr(self, bias_name)
    all_O = getattr(self, output_name)

    num_I = all_I.shape[0]
    num_K = (all_K.shape[0], all_K.shape[1])
    num_B = all_B.shape[0]
    num_O = all_O.shape[0]
    assert(num_I == num_K[0])
    assert(num_O == num_K[1] == num_B)
    assert(all_B.shape[1] == 1)

    for p in range(num_I):
      for q in range(num_O):
        I = all_I[p]
        K = all_K[p][q]
        B = all_B[q]
        O = all_O[q]

        # REFERENCE FORMULA: (6)
        # Verify the shape requirements for correlation with no padding.
        x_i, y_i = I.shape
        x_k, y_k = K.shape
        x_o, y_o = O.shape
        assert(x_o == x_i + x_k - 1)
        assert(y_o == y_i + y_k - 1)

        # REFERENCE FORMULA: (3) + (4)
        # Iterate through output matrix.
        for i in range(x_o):
          for j in range(y_o):
            O[i][j] = 0
            # Iterate through kernel.
            for u in range(x_k):
              for v in range(y_k):
                # Only care about indices within the bounds of input matrix
                # (because those outside the bounds are 0 anyway).
                if ((0 <= i - u) and (i - u <= x_i - 1) and
                    (0 <= j - v) and (j - v <= y_i - 1)):
                  O[i][j] += I[i - u][j - v] * K[u][v]

        all_O[q] = O

    # Update output.
    setattr(self, output_name, O)

  ###########################################
  #####             Sigmoid             #####
  ###########################################

  def sigmoid(self, input_name, output_name):

    # Get input and output.
    I = getattr(self, input_name)
    O = getattr(self, output_name)

    # Verify the shapes are consistent.
    assert(O.shape == I.shape)

    # REFERENCE FORMULA: (7)
    O = 1 / (1 + np.exp(-I))

    # Update output.
    setattr(self, output_name, O)

  # REFERENCE FORMULA: (8)
  def sigmoid_derivative(self, input_name, output_name):

    # Get input and output.
    I = getattr(self, input_name)
    O = getattr(self, output_name)

    # Verify the shapes are consistent.
    assert(O.shape == I.shape)

    # REFERENCE FORMULA: (8)
    O = np.multiply(I, (1 - I))

    # Update output.
    setattr(self, output_name, O)

  ###########################################
  #####             Pooling             #####
  ###########################################
  def pooling(self, input_name, output_name):

    # Get input and output.
    I = getattr(self, input_name)
    O = getattr(self, output_name)

    # Make sure the side length of output is half the side length of the input.
    # Assume the side length of the input is even.
    x_i, y_i = I.shape
    x_o, y_o = O.shape
    assert(x_i % 2 == 0 and y_i % 2 == 0)
    assert(x_i == 2 * x_o)
    assert(y_i == 2 * y_o)

    # REFERENCE FORMULA: (12) + (16)
    # Iterate through the output image.
    for i in range(x_o):
      for j in range(y_o):
        O[i][j] = 0
        # Iterate through the pooling patch.
        for u in range(1):
          for v in range(1):
            O[i][j] += 0.25 * (in_mat[2*i + u][2*j + v])

    # Update output.
    setattr(self, output_name, O)

  ###########################################
  #####          Vectorization          #####
  ###########################################
  def vectorize(self, input_name, output_name):

    # Get input and output.
    I = getattr(self, input_name)
    O = getattr(self, output_name)

    # Verify that the output is a vector.
    assert(len(O.shape) == 2)
    assert(O.shape[1] == 1)

    # Verify that the number of cells in the input matrix is equal to the
    # length of the output vector.
    assert(np.prod(I.shape) == O.shape[0])

    O = I.reshape(O.shape)

    # Update output.
    setattr(self, output_name, O)

  def unvectorize(self, input_name, output_name):

    # Get input and output.
    I = getattr(self, input_name)
    O = getattr(self, output_name)

    # Verify that the input is a vector.
    assert(len(I.shape) == 2)
    assert(I.shape[1] == 1)

    # Verify that the number of cells in the output matrix is equal to the
    # length of the input vector.
    assert(np.prod(O.shape) == I.shape[0])

    O = I.reshape(O.shape)

    # Update output.
    setattr(self, output_name, O)

  ###########################################
  #####         Fully Connected         #####
  ###########################################
  def fully_connected(self, input_name, weight_name, bias_name, output_name):
    # Get input, weight, bias, and output.
    I = getattr(self, input_name)
    W = getattr(self, weight_name)
    B = getattr(self, bias_name)
    O = getattr(self, output_name)

    # Verify the shapes are consistent.
    assert(O.shape[0] == W.shape[0] == B.shape[0])
    assert(O.shape[1] == I.shape[1] == B.shape[1])
    assert(W.shape[1] == I.shape[0])

    O = np.dot(W, I) + B

    # Update output.
    setattr(self, output_name, O)

  ###########################################
  #####          Loss Function          #####
  ###########################################
  def loss(self, truth_name, prediction_name, loss_name):
    # Get truth and prediction.
    truth = getattr(self, truth_name)
    prediction = getattr(self, prediction_name)
    L = getattr(self, loss_name)

    # Verify the shapes are consistent.
    assert(truth.shape == prediction.shape)

    L = 0.5 * np.sum(np.square((truth - prediction)))

    # Update output.
    setattr(self, loss_name, L)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FEEDFORWARD ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
  def feed_forward(self, X, Y):
    assert(self.X[0].shape == X.shape)
    assert(self.Y_hat.shape == Y.shape)

    ###########################################
    ######      Convolution Layer 1      ######
    ###########################################

    self.correlation("X", "K1", "B1", "C1")
    self.sigmoid("C1", "C1_sigma")

    # ###########################################
    # ######        Pooling Layer 1        ######
    # ###########################################
    # for i in range(S1.shape[0]):
    #   S1[i] = pooling(C1[i], S1[i])

    # ###########################################
    # ######      Convolution Layer 2      ######
    # ###########################################
    # for i in range(C2.shape[0]):
    #   temp = np.zeros(C2[i].shape)
    #   for j in range(S1.shape[0]):
    #     temp = correlation(S1[j], K2[j][i], temp)
    #     C2[i] += temp
    #   C2[i] = sigmoid(C2[i] + B2[i])

    # ###########################################
    # ######        Pooling Layer 2        ######
    # ###########################################
    # for i in range(S2.shape[0]):
    #   S2[i] = pooling(C2[i], S2[i])

    # ###########################################
    # ######         Vectorization         ######
    # ###########################################
    # F = vectorize(S2)

    # ###########################################
    # ######     Fully Connected Layer     ######
    # ###########################################
    # y_hat = sigmoid(fully_connected(F, W, B))

myCNN = CNN()
myCNN.feed_forward(myCNN.X_train[0], myCNN.Y_train[0])
