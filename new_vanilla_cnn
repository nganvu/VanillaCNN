#!/usr/bin/python3

import sys, os
import numpy as np

class CNN:

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#~~~~~~~~~~~~~~~~~~~~~~~~~~ INITIALIZING PARAMETERS ~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

  # Load MNIST dataset.
  def load_MNIST(self):
    sys.stderr = open(os.devnull, 'w') # Disable stderr.
    from keras.datasets import mnist
    (self.X_train, self.Y_train), (self.X_test, self.Y_test) = mnist.load_data()
    self.Y_train = self.convert_to_one_hot(self.Y_train)
    self.Y_test = self.convert_to_one_hot(self.Y_test)
    sys.stderr = sys.__stderr__ # Enable stderr.

  # Convert labels to one-hot (10 x 1) vectors.
  def convert_to_one_hot(self, labels):
      labels_vecs = np.zeros((labels.size, 10))
      labels_vecs[np.arange(labels.size), labels] = 1
      return labels_vecs

  # Initialize an array of a certain shape so that its values are uniformly
  # distributed on [-bound, bound].
  def initialize_uniformly(self, shape, bound):
    return self.scale_to_bound(np.random.rand(*shape), bound)

  # Given an array of values uniformly distributed on [0, 1], scale these values
  # so that they are uniformly distributed on [-bound, bound].
  def scale_to_bound(self, arr, bound):
    return arr * 2 * bound - bound

  def __init__(self):
    #################################
    ########## Load Inputs ##########
    #################################
    self.X_train = None
    self.Y_train = None
    self.X_test = None
    self.Y_test = None
    self.load_MNIST()

    # Input holder has the shape of one data point from training/testing data.
    self.X = np.zeros(self.X_train[0].shape)
    
    #################################
    ##### Convolutional Layer 1 #####
    #################################
    # (1 x 6) kernels of size (5 x 5).
    K1_shape = (1, 6, 5, 5)
    K1_bound = np.sqrt(6 / ((1 + 6) * (5 * 5)))
    self.K1 = self.initialize_uniformly(K1_shape, K1_bound)
    self.delta_K1 = np.zeros(self.K1.shape)

    # 6 biases.
    self.B1 = np.zeros((6, 1))
    self.delta_B1 = np.zeros(self.B1.shape)

    # 6 convolution outputs of size (24 x 24), before and after sigmoid.
    self.C1 = np.zeros((6, 24, 24))
    self.C1_sigma = np.zeros(self.C1.shape)
    self.delta_C1 = np.zeros(self.C1.shape)
    self.delta_C1_sigma = np.zeros(self.C1_sigma.shape)

    #################################
    #####    Pooling Layer 1    #####
    #################################
    # 6 sampling outputs of size (12 x 12).
    self.S1 = np.zeros((6, 12, 12))
    self.delta_S1 = np.zeros(self.S1.shape)

    #################################
    ##### Convolutional Layer 2 #####
    #################################
    # (6 x 12) kernels of size (5 x 5).
    K2_shape = (6, 12, 5, 5)
    K2_bound = np.sqrt(6 / ((6 + 12) * (5 * 5)))
    self.K2 = self.initialize_uniformly(K2_shape, K2_bound)
    self.delta_K2 = np.zeros(self.K2.shape)

    # 12 biases.
    self.B2 = np.zeros((12, 1))
    self.delta_B2 = np.zeros(self.B2.shape)

    # 12 convolution outputs of size (8 x 8), before and after sigmoid.
    self.C2 = np.zeros((12, 8, 8))
    self.C2_sigma = np.zeros(self.C2.shape)
    self.delta_C2 = np.zeros(self.C2.shape)
    self.delta_C2_sigma = np.zeros(self.C2_sigma.shape)

    #################################
    #####    Pooling Layer 2    #####
    #################################
    # 12 sampling outputs of size (4 x 4).
    self.S2 = np.zeros((12, 4, 4))
    self.delta_S2 = np.zeros(self.S2.shape)

    #################################
    ##### Fully Connected Layer #####
    #################################
    # Vectorization of 12 matrices of size (4 x 4) to a vector of size 192.
    self.V = np.zeros((192, 1))
    self.delta_V = np.zeros(self.V.shape)

    # 10 weights of size 192.
    W_shape = (10, 192)
    W_bound = np.sqrt(6 / (192 + 10))
    self.W = self.initialize_uniformly(W_shape, W_bound)
    self.delta_W = np.zeros(self.W.shape)

    # 10 biases.
    self.B = np.zeros((10, 1))
    self.delta_B = np.zeros(self.B.shape)

    # 10 output classes, before and after sigmoid.
    self.Y = np.zeros((10, 1))
    self.Y_sigma = np.zeros(self.Y.shape)
    self.delta_Y = np.zeros(self.Y.shape)
    self.delta_Y_sigma = np.zeros(self.delta_Y.shape)

    # Loss.
    self.loss = 0.0

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~ UNIVERSAL FUNCTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#

  #################################
  #####      Correlation      #####
  ##### (padding is not used) #####
  #################################

  def correlation(self, input_name, kernel_name, output_name):

    # Get input, kernel, and output.
    I = getattr(self, input_name)
    K = getattr(self, kernel_name)
    O = getattr(self, output_name)
    
    # REFERENCE FORMULA: (5)
    # Verify the shape requirements for correlation with no padding.
    x_i, y_i = I.shape
    x_k, y_k = K.shape
    x_o, y_o = O.shape
    assert(x_o == x_i - x_k + 1)
    assert(y_o == y_i - y_k + 1)

    # REFERENCE FORMULA: (1)
    # Iterate through output matrix.
    for i in range(x_o):
      for j in range(y_o):
        O[i][j] = 0
        # Iterate through kernel.
        for u in range(x_k):
          for v in range(y_k):
            O[i][j] += I[i + u][j + v] * K[u][v]

    # Update output.
    setattr(self, output_name, O)

  #################################
  #####      Convolution      #####
  #####   (padding is used)   #####
  #################################

  def convolution(self, input_name, kernel_name, output_name):

    # Get input, kernel, and output.
    I = getattr(self, input_name)
    K = getattr(self, kernel_name)
    O = getattr(self, output_name)
    
    # REFERENCE FORMULA: (6)
    # Verify the shape requirements for correlation with no padding.
    x_i, y_i = I.shape
    x_k, y_k = K.shape
    x_o, y_o = O.shape
    assert(x_o == x_i + x_k - 1)
    assert(y_o == y_i + y_k - 1)

    # REFERENCE FORMULA: (3) + (4)
    # Iterate through output matrix.
    for i in range(x_o):
      for j in range(y_o):
        O[i][j] = 0
        # Iterate through kernel.
        for u in range(x_k):
          for v in range(y_k):
            # Only care about indices within the bounds of input matrix
            # (because those outside the bounds are 0 anyway).
            if ((0 <= i - u) and (i - u <= x_i - 1) and
                (0 <= j - v) and (j - v <= y_i - 1)):
              O[i][j] += I[i - u][j - v] * K[u][v]
            
    # Update output.
    setattr(self, output_name, O)

  #################################
  #####        Sigmoid        #####
  #################################
  
  def sigmoid(self, input_name, output_name):

    # Get input and output.
    I = getattr(self, input_name)
    O = getattr(self, output_name)

    # Verify the shapes are consistent.
    assert(O.shape == I.shape)

    # REFERENCE FORMULA: (7)
    O = 1 / (1 + np.exp(-I))

    # Update output.
    setattr(self, output_name, O)

  # REFERENCE FORMULA: (8)
  def sigmoid_derivative(self, input_name, output_name):
    
    # Get input and output.
    I = getattr(self, input_name)
    O = getattr(self, output_name)

    # Verify the shapes are consistent.
    assert(O.shape == I.shape)

    # REFERENCE FORMULA: (8)
    O = np.multiply(I, (1 - I))

    # Update output.
    setattr(self, output_name, O)

  #################################
  #####        Pooling        #####
  #################################
  def pooling(self, input_name, output_name):

    # Get input and output.
    I = getattr(self, input_name)
    O = getattr(self, output_name)

    # Make sure the side length of output is half the side length of the input.
    # Assume the side length of the input is even.
    x_i, y_i = I.shape
    x_o, y_o = O.shape
    assert(x_i % 2 == 0 and y_i % 2 == 0)
    assert(x_i == 2 * x_o)
    assert(y_i == 2 * y_o)

    # REFERENCE FORMULA: (12) + (16)
    # Iterate through the output image.
    for i in range(x_o):
      for j in range(y_o):
        O[i][j] = 0
        # Iterate through the pooling patch.
        for u in range(1):
          for v in range(1):
            O[i][j] += 0.25 * (in_mat[2*i + u][2*j + v])

    # Update output.
    setattr(self, output_name, O)

  #################################
  #####     Vectorization     #####
  #################################
  def vectorize(self, input_name, output_name):

    # Get input and output.
    I = getattr(self, input_name)
    O = getattr(self, output_name)

    # Verify that the output is a vector.
    assert(len(O.shape) == 2)
    assert(O.shape[1] == 1)

    # Verify that the number of cells in the input matrix is equal to the
    # length of the output vector.
    assert(np.prod(I.shape) == O.shape[0])

    O = I.reshape(O.shape)

    # Update output.
    setattr(self, output_name, O)

  def unvectorize(self, input_name, output_name):

    # Get input and output.
    I = getattr(self, input_name)
    O = getattr(self, output_name)

    # Verify that the input is a vector.
    assert(len(I.shape) == 2)
    assert(I.shape[1] == 1)

    # Verify that the number of cells in the output matrix is equal to the
    # length of the input vector.
    assert(np.prod(O.shape) == I.shape[0])

    O = I.reshape(O.shape)

    # Update output.
    setattr(self, output_name, O)

  #################################
  #####    Fully Connected    #####
  #################################
  def fully_connected(self, input_name, weight_name, bias_name, output_name):
    # Get input, weight, bias, and output.
    I = getattr(self, input_name)
    W = getattr(self, weight_name)
    B = getattr(self, bias_name)
    O = getattr(self, output_name)

    # Verify the shapes are consistent.
    assert(O.shape[0] == W.shape[0] == B.shape[0])
    assert(O.shape[1] == I.shape[1] == B.shape[1])
    assert(W.shape[1] == I.shape[0])

    O = np.dot(W, I) + B

    # Update output.
    setattr(self, output_name, O)

  #################################
  #####     Loss Function     #####
  #################################
  def loss(self, truth_name, prediction_name, loss_name):
    # Get truth and prediction.
    truth = getattr(self, truth_name)
    prediction = getattr(self, prediction_name)
    L = getattr(self, loss_name)

    # Verify the shapes are consistent.
    assert(truth.shape == prediction.shape)

    L = 0.5 * np.sum(np.square((truth - prediction)))

    # Update output.
    setattr(self, loss_name, L)

print("Fully connected test")
myCNN = CNN()
myCNN.V = np.random.rand(*myCNN.V.shape)
myCNN.B = np.random.rand(*myCNN.B.shape)
myCNN.fully_connected("V", "W", "B", "Y")
print(myCNN.Y)

print("Vectorize test")
myCNN.S2 = np.random.rand(*myCNN.S2.shape)
old_S2 = myCNN.S2
myCNN.vectorize("S2", "V")
myCNN.S2 = np.zeros(myCNN.S2.shape)
myCNN.unvectorize("V", "S2")
print(np.array_equal(old_S2, myCNN.S2))

sys.exit()

print("Sigmoid test")
myCNN.C1 = np.array([[-4, -3, -2, -1,  0],
                    [-3, -2, -1,  0,  1],
                    [-2, -1,  0,  1,  2],
                    [-1,  0,  1,  2,  3],
                    [ 0,  1,  2,  3,  4]])
myCNN.C1_sigma = np.zeros(myCNN.C1.shape)
print(myCNN.C1)
myCNN.sigmoid("C1", "C1_sigma")
print(myCNN.C1_sigma)
myCNN.sigmoid_derivative("C1_sigma", "C1")
print(myCNN.C1)

print("Correlation Test")

myCNN.X = in_mat = np.random.rand(8, 8)
myCNN.C1 = out_mat = np.zeros((12, 12))
myCNN.K = kernel = np.random.rand(5, 5)
print("Input")
print(in_mat)
print("Kernel")
print(kernel)

def baseline_convolution(in_mat, kernel, out_mat):
    s = kernel.shape + tuple(np.subtract(in_mat.shape, kernel.shape) + 1)
    subM = np.lib.stride_tricks.as_strided(
    in_mat, shape = s, strides = in_mat.strides * 2)
    return np.einsum("ij,ijkl->kl", kernel, subM)
print("Baseline:")
print(baseline_convolution(in_mat, np.rot90(kernel, 2), out_mat))

print("Mine:")
myCNN.convolution("I", "K", "C1")
print(myCNN.C1[4:8, 4:8]) # The baseline convolution does not use padding.